#!/usr/bin/env python
from urllib3.contrib.socks import SOCKSProxyManager
import urllib3
from urllib3.util.retry import MaxRetryError
from six.moves.urllib.request import urlopen
from threading import Thread
from collections import defaultdict
from argparse import ArgumentParser
import os
import time
import logging
import json
from random import shuffle
import zlib

from .database import Check, init_database

# https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
urllib3.disable_warnings()

THREADS = 50 
REPEAT = 1
# http://urllib3.readthedocs.io/en/latest/reference/urllib3.util.html#module-urllib3.util.timeout
CONNECT_TIMEOUT = 1
READ_TIMEOUT = 3 # this does NOT limit total read time


def download_plist(url):
    res = urlopen(url)
    lines = res.read().decode('ascii').splitlines()
    return lines


def check_proxy(proxy, proxy_type):
    if proxy_type == 'socks':
        pool = SOCKSProxyManager('socks5://%s' % proxy)
    else:
        pool = urllib3.ProxyManager('http://%s' % proxy)
    retries = urllib3.Retry(connect=0, read=0,
                            redirect=10,
                            raise_on_redirect=False)
    timeout = urllib3.Timeout(connect=CONNECT_TIMEOUT,
                              read=READ_TIMEOUT)
    op = {
        'status': None,
        'connect_time': None,
        'read_time': None,
        'error': None,
    }
    try:
        start_time = time.time()
        res = pool.request('GET', 'http://en.wikipedia.org/robots.txt',
                           retries=retries, timeout=timeout,
                           preload_content=False)
        connected_time = time.time()
        op['connect_time'] = round(connected_time - start_time, 2)
        data = res.read()
        op['read_time'] = round(time.time() - connected_time, 2)
    except Exception as ex:
        if isinstance(ex, MaxRetryError):
            ex = ex.reason
        error = type(ex).__name__
        op['error'] = error
        if error in ('ProxyError', 'NewConnectionError', 'ConnectTimeoutError'):
            op['status'] = 'connect_fail' 
        elif error in ('ProtocolError', 'ReadTimeoutError',):
            op['status'] = 'read_fail'
        else:
            logging.error('', exc_info=ex)
            raise Exception('Unexpected error: %s' % error)
    else:
        if b'# Wikipedia work bots:' in data:
            op['status'] = 'ok'
        else:
            op['status'] = 'data_fail'
    return op


def check_worker(task_iter, proxy_type, repeat, stat):
    while True:
        try:
            proxy = next(task_iter)
        except StopIteration:
            break
        else:
            for x in range(repeat):
                op = check_proxy(proxy, proxy_type)
                stat['count'][op['status']] += 1
                stat['ops'][proxy].append(op)
                if op['status'] == 'ok':
                    stat['count']['ok_connect_time'] += op['connect_time']
                    stat['count']['ok_read_time'] += op['read_time']


def get_stat_fails(stat):
    return sum(stat['count'][x] for x in ['connect_fail', 'read_fail',
                                          'data_fail'])


def render_stat_counts(name, stat):
    fail = get_stat_fails(stat)
    return ('[%s] OK: %d, FAIL: %d (CONNECT: %d, READ: %d, DATA: %d)' % (
            name,
            stat['count']['ok'],
            fail,
            stat['count']['connect_fail'],
            stat['count']['read_fail'],
            stat['count']['data_fail']))


def stat_worker(name, stat):
    while True:
        time.sleep(3)
        print(render_stat_counts(name, stat))


def normalize_plist_url(url):
    if not url.startswith(('http://', 'https://', 'file://')):
        url = os.path.join(os.path.abspath(os.getcwd()), url)
        url = 'file://localhost' + url
    return url


def check_plist(plist_url, proxy_type, threads=THREADS,
                limit=None, name=None, save=False, repeat=REPEAT,
                fail_file=None):
    if not name:
        name = plist_url.split('/')[-1]

    if save:
        init_database()
    plist_url = normalize_plist_url(plist_url)
    plist = download_plist(plist_url)
    shuffle(plist)

    def task_iter_func(plist, limit=None):
        for count, proxy in enumerate(plist):
            yield proxy
            if limit and (count + 1) >= limit:
                break

    task_iter = task_iter_func(plist, limit)
    stat = {
        'count': {
            'ok': 0,
            'connect_fail': 0,
            'read_fail': 0,
            'data_fail': 0,
            'ok_connect_time': 0,
            'ok_read_time': 0,
        },
        'ops': defaultdict(list),
    }
    
    th = Thread(target=stat_worker, args=[name, stat])
    th.daemon = True
    th.start()
    start = time.time()

    pool = []
    for x in range(threads):
        th = Thread(target=check_worker, args=[task_iter, proxy_type,
                                               repeat, stat])
        th.start()
        pool.append(th)
    for th in pool:
        th.join()
    session_time = time.time() - start
    print(render_stat_counts(name, stat))
    
    ops_blob = zlib.compress(json.dumps(stat['ops']).encode('utf-8'))

    if save:
        Check.create(
            name=name,
            count_ok=stat['count']['ok'],
            count_fail=get_stat_fails(stat),
            count_connect_fail=stat['count']['connect_fail'],
            count_read_fail=stat['count']['read_fail'],
            count_data_fail=stat['count']['data_fail'],
            avg_connect_time=round(stat['count']['ok_connect_time']
                                   / (stat['count']['ok'] or 1), 2),
            avg_read_time=round(stat['count']['ok_read_time']
                                / (stat['count']['ok'] or 1), 2),
            session_time=round(session_time, 2),
            ops=ops_blob,
        )
    if fail_file:
        rows = []
        for addr, ops in stat['ops'].items():
            for op in ops:
                if op['status'] != 'ok':
                    rows.append((op['status'], addr))
        with open(fail_file, 'w') as out:
            for row in sorted(rows, key=lambda x: x[0]):
                out.write('%s:%s\n' % row)



def main():
    parser = ArgumentParser()
    parser.add_argument('proxy_type')
    parser.add_argument('plist_url')
    parser.add_argument('-l', '--limit', type=int)
    parser.add_argument('-t', '--threads', default=THREADS, type=int)
    parser.add_argument('-n', '--name')
    parser.add_argument('-r', '--repeat', type=int, default=REPEAT)
    parser.add_argument('-s', '--save', action='store_true', default=False)
    parser.add_argument('--fail-file')
    opts = parser.parse_args()
    check_plist(opts.plist_url, limit=opts.limit, proxy_type=opts.proxy_type,
                save=opts.save, repeat=opts.repeat, fail_file=opts.fail_file)


if __name__ == '__main__':
    main()
